# general
data_dir: processed_data
gpu_config: configs/gpu_config_7b.json
cuda: 0
seed: 0
exps_path: exps
train: True
val: True
test: True

# frame processing
flip_p: 0.5

# encoder
use_vqvae: False
use_clip: openai/clip-vit-large-patch14
freeze_encoder: True
encoder_path: models/clip/trained_clip/encoder.pt
encoder_output_size: 1024
num_context_vision: 8
prompt_depth_vision: 12
dim_context_vision: 1024
num_context_text: 4
prompt_depth_text: 12
dim_context_text: 768

# projection
freeze_projection: False
projection_lr: 0.0002
projection_path: exps/phi_3_newclip_full_allign/project.pt # Null for feature allignment

# LLM
train_files: [processed_data/train_qa.json]
val_files: [processed_data/val_opd_qa.json]
test_files: [processed_data/test_qa.json, processed_data/test_opd_qa.json]
model_type: phi-3 #instead of vicuna-7b
cutoff_len: 512
offload_dir: models/phi3_5/offload # offload
llm_lr: 0.0002
quantized: False
bnb_4bit: False
tokenizer_path: exps/phi_3_newclip_full_allign/tokenizer # Null for feature allignment
llm_path: exps/phi_3_newclip_full_allign/llm_weights # Null for feature allignment
## LoRA
lora_trained: False
use_lora: True # False for feature allignment
lora_alpha: 64
r: 32
lora_dropout: 0.05
target_modules: # Phi-3
  - qkv_proj

modules_to_save:
  - embed_tokens
bias: none
## train
max_train_steps: 3000 # 10000 for feature allignment, 3000 for end-to-end fine tuning
save_freq: null
per_device_train_batch_size: 1
llm_gradient_accumulation_steps: 16
warmup_steps: 0.03
## val
per_device_val_batch_size: 1
## generation
max_new_tokens:
  train_object_property_description: 100
  train_object_description: 100
  train_property_comparison: 200
  train_property_superlative_selection: 200
  train_property_object_match: 200
  eval_object_property_description: 100
  eval_property_comparison: 150
  eval_property_superlative_selection: 200
  eval_property_superlative_selection_most: 200
  eval_property_superlative_selection_least: 200
  eval_property_object_match: 200
  eval_property_scenario_reasoning: 200